{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb466db-fe54-47a3-aa02-c9cc15d77e70",
      "metadata": {
        "id": "4eb466db-fe54-47a3-aa02-c9cc15d77e70"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9d167d5-8d8c-49a5-a1d9-5137cba2b7fb",
      "metadata": {
        "id": "f9d167d5-8d8c-49a5-a1d9-5137cba2b7fb"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('model_data.csv')\n",
        "val_data = pd.read_csv('valid_data.csv')\n",
        "val_data['url'] = val_data['url'].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5709decf-7889-4867-9bc5-3e0863d83043",
      "metadata": {
        "id": "5709decf-7889-4867-9bc5-3e0863d83043",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(f'정상 url 비율 = {round(data[\"Label\"].value_counts()[0]/len(data) * 100,3)}%')\n",
        "print(f'피싱 url 비율 = {round(data[\"Label\"].value_counts()[1]/len(data) * 100,3)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021afe78-7baa-4cdd-9f9b-a07502a736fd",
      "metadata": {
        "id": "021afe78-7baa-4cdd-9f9b-a07502a736fd"
      },
      "outputs": [],
      "source": [
        "print(f'정상 url 비율 = {round(val_data[\"Label\"].value_counts()[0]/len(val_data) * 100,3)}%')\n",
        "print(f'피싱 url 비율 = {round(val_data[\"Label\"].value_counts()[1]/len(val_data) * 100,3)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a022ac-ac16-4ce1-bcb1-cce1106e03bb",
      "metadata": {
        "id": "c6a022ac-ac16-4ce1-bcb1-cce1106e03bb"
      },
      "outputs": [],
      "source": [
        "#독립 변수\n",
        "X_data = data['url']\n",
        "\n",
        "# 종속 변수\n",
        "y_data = data['Label']\n",
        "\n",
        "# 검증용 데이터\n",
        "shuffled_val_data = val_data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "X_val = shuffled_val_data['url']\n",
        "y_val = shuffled_val_data['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45c3649c-b33e-4c83-b975-262d1bf6b865",
      "metadata": {
        "id": "45c3649c-b33e-4c83-b975-262d1bf6b865"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, stratify=y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1afd94-9ca4-4408-afa1-6515ebebd513",
      "metadata": {
        "id": "8c1afd94-9ca4-4408-afa1-6515ebebd513"
      },
      "outputs": [],
      "source": [
        "# 각 데이터의 크기 확인\n",
        "print(\"훈련 데이터 크기:\", X_train.shape, y_train.shape)\n",
        "print(\"검증 데이터 크기:\", X_val.shape, y_val.shape)\n",
        "print(\"테스트 데이터 크기:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fd0d74-7bb6-47b1-a793-40b0ab7ec13d",
      "metadata": {
        "id": "27fd0d74-7bb6-47b1-a793-40b0ab7ec13d"
      },
      "outputs": [],
      "source": [
        "print('--------훈련 데이터의 비율-----------')\n",
        "print(f'정상 url = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n",
        "print(f'스팸 url = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')\n",
        "\n",
        "print('--------테스트 데이터의 비율-----------')\n",
        "print(f'정상 url = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')\n",
        "print(f'스팸 url = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')\n",
        "\n",
        "print('--------검증 데이터의 비율-----------')\n",
        "print(f'정상 url = {round(y_val.value_counts()[0]/len(y_val) * 100,3)}%')\n",
        "print(f'스팸 url = {round(y_val.value_counts()[1]/len(y_val) * 100,3)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04bdd33-75f1-4835-9c6c-813e1562f49e",
      "metadata": {
        "id": "b04bdd33-75f1-4835-9c6c-813e1562f49e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# URL을 구성 요소로 분리하는 함수\n",
        "def tokenize_url(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    scheme = parsed_url.scheme\n",
        "    netloc = parsed_url.netloc\n",
        "    path = parsed_url.path\n",
        "\n",
        "    # 구성 요소를 리스트로 결합\n",
        "    url_parts = [scheme] + netloc.split('.') + path.split('/')\n",
        "\n",
        "    # 빈 문자열 제거\n",
        "    url_parts = [part for part in url_parts if part]\n",
        "\n",
        "    return url_parts\n",
        "\n",
        "# URL 리스트를 구성 요소로 분리\n",
        "X_train_list = [tokenize_url(url) for url in X_train]\n",
        "\n",
        "# 구성 요소를 문자열로 결합\n",
        "X_train_list = [' '.join(parts) for parts in X_train_list]\n",
        "\n",
        "# Tokenizer 초기화 및 텍스트 적합화\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train_list)\n",
        "\n",
        "# 저장\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# 텍스트를 시퀀스로 변환\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train_list)\n",
        "\n",
        "print(\"토큰화된 URL 구성 요소:\", X_train_encoded[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d529b26c-f19a-4be0-b9bd-af8af63cb06e",
      "metadata": {
        "id": "d529b26c-f19a-4be0-b9bd-af8af63cb06e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "word_to_index = tokenizer.word_index\n",
        "word_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787cb426-6fc9-4dca-b03e-350f8e3aaefc",
      "metadata": {
        "id": "787cb426-6fc9-4dca-b03e-350f8e3aaefc"
      },
      "outputs": [],
      "source": [
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d40c22-af5e-4674-89d3-dc1a23303165",
      "metadata": {
        "id": "08d40c22-af5e-4674-89d3-dc1a23303165"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(word_to_index) + 1\n",
        "print('단어 집합의 크기: {}'.format((vocab_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99c91072-3feb-4ebd-bde1-9ba24b0cf400",
      "metadata": {
        "id": "99c91072-3feb-4ebd-bde1-9ba24b0cf400"
      },
      "outputs": [],
      "source": [
        "print('url 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n",
        "print('url 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df86d2e-4b24-4bf8-bfe0-eb8ebc666bd1",
      "metadata": {
        "id": "1df86d2e-4b24-4bf8-bfe0-eb8ebc666bd1"
      },
      "outputs": [],
      "source": [
        "max_len = 560\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da9781d-f4b5-4309-9ae7-7667d2d4d5e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "9da9781d-f4b5-4309-9ae7-7667d2d4d5e3",
        "outputId": "4d468f1d-89c7-4eb9-9bae-ff75618e50df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - acc: 0.9294 - loss: 0.1849\n",
            "Epoch 1: val_acc improved from -inf to 0.97750, saving model to best_model.keras\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 141ms/step - acc: 0.9294 - loss: 0.1849 - val_acc: 0.9775 - val_loss: 0.0709\n",
            "Epoch 2/5\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - acc: 0.9906 - loss: 0.0298\n",
            "Epoch 2: val_acc improved from 0.97750 to 0.98239, saving model to best_model.keras\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 139ms/step - acc: 0.9906 - loss: 0.0298 - val_acc: 0.9824 - val_loss: 0.0621\n",
            "Epoch 3/5\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - acc: 0.9984 - loss: 0.0064\n",
            "Epoch 3: val_acc did not improve from 0.98239\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 140ms/step - acc: 0.9984 - loss: 0.0064 - val_acc: 0.9766 - val_loss: 0.0994\n",
            "Epoch 4/5\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - acc: 0.9992 - loss: 0.0031\n",
            "Epoch 4: val_acc improved from 0.98239 to 0.98754, saving model to best_model.keras\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 139ms/step - acc: 0.9992 - loss: 0.0031 - val_acc: 0.9875 - val_loss: 0.0493\n",
            "Epoch 5/5\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - acc: 0.9994 - loss: 0.0022\n",
            "Epoch 5: val_acc did not improve from 0.98754\n",
            "\u001b[1m2900/2900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 140ms/step - acc: 0.9994 - loss: 0.0022 - val_acc: 0.9861 - val_loss: 0.0597\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 32\n",
        "dropout_ratio = 0.3\n",
        "num_filters = 32\n",
        "kernel_size = 5\n",
        "\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(64, return_sequences=False))  # 64는 유닛 수\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, embedding_dim))\n",
        "# model.add(Dropout(dropout_ratio))\n",
        "# model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "# model.add(Dropout(dropout_ratio))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('best_model.keras', monitor = 'val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[es, mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88be56d-6bc9-41c3-9351-43597f1d28f3",
      "metadata": {
        "id": "d88be56d-6bc9-41c3-9351-43597f1d28f3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "# 1. 테스트 데이터 예측 (확률값)\n",
        "y_pred_proba = model.predict(X_test_padded).flatten()  # 예측 확률값\n",
        "\n",
        "# 2. 이진 분류 결과 생성 (0.5 기준)\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# 3. 성능 지표 계산\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)  # ROC AUC은 확률값 기반\n",
        "\n",
        "# 4. 결과 출력\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f648b86-0435-4ba5-ab47-792a8c780ea8",
      "metadata": {
        "id": "8f648b86-0435-4ba5-ab47-792a8c780ea8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def preprocess_url(url, tokenizer, max_len=560):\n",
        "    # Tokenize and convert to string\n",
        "    tokenized_url = tokenize_url(url)\n",
        "    url_string = ' '.join(tokenized_url)\n",
        "\n",
        "    # Encode to sequence\n",
        "    encoded_url = tokenizer.texts_to_sequences([url_string])\n",
        "\n",
        "    # Pad sequence\n",
        "    padded_url = pad_sequences(encoded_url, maxlen=max_len)\n",
        "\n",
        "    return padded_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d60518-0708-4b93-8d4b-0817f1ff4ed8",
      "metadata": {
        "id": "15d60518-0708-4b93-8d4b-0817f1ff4ed8"
      },
      "outputs": [],
      "source": [
        "def tokenize_url(url):\n",
        "    from urllib.parse import urlparse\n",
        "    parsed_url = urlparse(url)\n",
        "    scheme = parsed_url.scheme\n",
        "    netloc = parsed_url.netloc\n",
        "    path = parsed_url.path\n",
        "\n",
        "    # 구성 요소 분리 및 리스트 생성\n",
        "    url_parts = [scheme] + netloc.split('.') + path.split('/')\n",
        "    url_parts = [part for part in url_parts if part]  # 빈 문자열 제거\n",
        "\n",
        "    return url_parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fbfce6-9b23-4ddb-aa3f-2a2c6c56f173",
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "c5fbfce6-9b23-4ddb-aa3f-2a2c6c56f173"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# URL 텍스트를 벡터화하기 위해 TF-IDF 적용\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)  # 텍스트 벡터화\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# LGBM 모델 학습\n",
        "lgbm_model = LGBMClassifier(max_depth=5, num_leaves=31, learning_rate=0.05, n_estimators=100, random_state=0)\n",
        "lgbm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "lgbm_proba = lgbm_model.predict_proba(X_test_tfidf)[:, 1]  # 확률 예측\n",
        "lgbm_pred = (lgbm_proba >= 0.5).astype(int)  # 클래스 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0d53d8-fd5c-47b0-9150-4d900fdc085d",
      "metadata": {
        "id": "af0d53d8-fd5c-47b0-9150-4d900fdc085d",
        "outputId": "c1ba4e5a-c1ae-4ead-a635-e11997f928f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1813/1813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 32ms/step\n",
            "\u001b[1m7250/7250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 33ms/step\n",
            "XGBoost Stacking - Accuracy: 0.9865\n",
            "XGBoost Stacking - F1 Score: 0.9859\n",
            "XGBoost Stacking - Precision: 0.9934\n",
            "XGBoost Stacking - Recall: 0.9786\n",
            "XGBoost Stacking - ROC AUC: 0.9976\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. LSTM 모델 예측 확률\n",
        "lstm_proba = model.predict(X_test_padded).flatten()\n",
        "\n",
        "# 2. LGBM 모델 예측 확률\n",
        "lgbm_proba = lgbm_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# 3. Stacking 데이터 생성\n",
        "stacked_train = np.column_stack((lgbm_model.predict_proba(X_train_tfidf)[:, 1],\n",
        "                                 model.predict(X_train_padded).flatten()))\n",
        "stacked_test = np.column_stack((lgbm_proba, lstm_proba))\n",
        "\n",
        "# 4. XGBoost 메타 모델 학습\n",
        "meta_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, g=0)\n",
        "meta_model.fit(stacked_train, y_train)\n",
        "\n",
        "# 5. XGBoost 메타 모델 예측\n",
        "stacked_pred = meta_model.predict(stacked_test)\n",
        "stacked_proba = meta_model.predict_proba(stacked_test)[:, 1]\n",
        "\n",
        "# 6. 평가\n",
        "accuracy = accuracy_score(y_test, stacked_pred)\n",
        "f1 = f1_score(y_test, stacked_pred)\n",
        "precision = precision_score(y_test, stacked_pred)\n",
        "recall = recall_score(y_test, stacked_pred)\n",
        "roc_auc = roc_auc_score(y_test, stacked_proba)\n",
        "\n",
        "print(f\"XGBoost Stacking - Accuracy: {accuracy:.4f}\")\n",
        "print(f\"XGBoost Stacking - F1 Score: {f1:.4f}\")\n",
        "print(f\"XGBoost Stacking - Precision: {precision:.4f}\")\n",
        "print(f\"XGBoost Stacking - Recall: {recall:.4f}\")\n",
        "print(f\"XGBoost Stacking - ROC AUC: {roc_auc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}